\documentclass[10pt,a4paper,final]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\title{A quick summary of the K-SVD algorithm}
\author{Devdeep Ray}
\begin{document}
\maketitle
\section{Introduction}
We discuss the K-SVD algorithm in brief. This algorithm aims to select a dictionary that best represents a set of given training samples under sparsity constraints. It is a two phase iterative algorithm that alternates between sparse coding of the samples and updating the dictionary.
\section{Getting sparse representation for a given dictionary}
Let $n$ be the dimensionality of the data and $K$ be the number of atoms in the dictionary. We want a matrix $\mathbf{D} \in \mathbb{R}^{n \times K}$ which gives the best possible reconstruction of $\mathbf{y} \in \mathbf{R}^n=\mathbf{Dx}$, under constraint that the weight vector $\mathbf{x} \in \mathbf{R}^n$ is sparse. Now, if $n < K$, and $\mathbf{D}$ has full rank, then we will have infinite solutions for $\mathbf{x}$. Thus, we try to choose the $\mathbf{x}$ with the least number of non zero coefficients (sparsity constraint). This can be put down as\\
\begin{center}
$\genfrac{}{}{0pt}{}{argmin}{\mathbf{x}}\|\mathbf{x}\|_0$ subject to $\|\mathbf{y} - \mathbf{Dx}\|_2 < \epsilon$ 
\end{center}
Here, $\|\cdot\|_0$ is the zero norm, which is basically the number of non-zero elements. This is one of the phases of the two step iterative process. This turns out to be an NP-hard problem and there are approximate solutions using various pursuit algorithms, eg. matching pursuit, orthogonal matching pursuit, etc.
\section{The dictionary}
In this report, we discuss an algorithm to get a dictionary from training data that gives sparse representation of the data. More specifically, we find the dictionary that gives the best possible representation of each member in the traning set but under strict sparsity constraints. The overall optimization problem can be stated in two ways: \\
\begin{center}
$\genfrac{}{}{0pt}{}{argmin}{\mathbf{D},\mathbf{X}}\{\|\mathbf{Y}-\mathbf{DX}\|_F^2\}$ subject to $\forall i, \|\mathbf{x}_i\|_0 \leq T_0$\\
OR\\
$\genfrac{}{}{0pt}{}{argmin}{\mathbf{D}, \mathbf{X}}\sum_i\|\mathbf{x}_i\|_0$ subject to $\|\mathbf{Y}-\mathbf{DX}\|_F^2 \leq \epsilon$\\
\end{center}
In the first case, we are getting sparsity by applying a hard limit on the number of non-zero elements, and trying to minimize the representative error. In the second one, we are applying a hard limit on the error, and trying to maximize sparsity by minimizing the number of non-zero elements. The paper only discusses the first case. \pagebreak
\section{The K-SVD algorithm}
\subsection{The two phases}
The algorithm proceeds in two phases. In one phase, the dictionary is fixed, and we get sparse vectors, $\mathbf{x}_i$, which result in the reconstructions having least MSE under the given dictionary. 
In the second phase, we update $\mathbf{D}$ column by column, fixing $\mathbf{X}$ and all other columns of $\mathbf{D}$. This update is immediately incorporated into the matrix $\mathbf{D}$, accelerating convergence. This is similar to the Gauss-Seidel algorithm, where the new updates are not delayed till the end of the macro step, but immediately applied and used for the update of the next column. Once this is done, $\mathbf{X}$ is recomputed and the algorithm keeps alternating like this.
\subsection{Dictionary update}
The error term can be rewritten as
\begin{center}
$\|\mathbf{Y}-\sum_{j=1}^{K}\mathbf{d}_j\mathbf{x}_T^j\|_F^2$
\end{center}
where $\mathbf{d}_jj$ is the $j^{th}$ column of $\mathbf{D}$ and $\mathbf{x}_T^j$ is the $j^{th}$ row of $\mathbf{X}$. This can further be written as 
\begin{center}
$\|\mathbf{E}_k - \mathbf{d}_k\mathbf{x}_T^k\|_F^2$
\end{center}
for some $k$ where 
\begin{center}
$\mathbf{E}_k=\left(\mathbf{Y}-\sum_{j \neq k} \mathbf{d}_j \mathbf{x}_T^j\right)$
\end{center}
Now, $\mathbf{d}_k\mathbf{x}_T^k$ is a rank 1 matrix, and we may use SVD to get this, as SVD gives the best rank 1 approximation of a matrix (under the Frobenius norm), but if we do that here, then the new $\mathbf{x}_T^k$ will probably be filled, hence destroying our sparsity constraint (consider a column where an element of $\mathbf{x}_T^k$ is zero, and the column is of maximum possible sparsity. If the new element at this column of $\mathbf{x}_T^k$ is non-zero, we have broken the sparsity constraint). 
To overcome this, we work with only those examples which use the $j^{th}$ atom in the current representation. Let the number of non-zero elements in $\mathbf{x}_T^j$ be $q$. Chose only these columns from $\mathbf{E}_k$ and call it $\mathbf{E}_k^R$. This is the error matrix corresponding to the examples which use atom $j$. Similarly, remove from $\mathbf{x}_T^j$ the zero elements and call this new row vector $\mathbf{x}_R^j$. 
Now, if we minimize $\|\mathbf{E}_k^R - \mathbf{d}_k\mathbf{x}_R^k\|_F^2$ wrt $\mathbf{d}_j$ and $\mathbf{x}_R^j$, it is like minimizing the original function, but with the same non-zero columns or a subset of them. This can be done by SVD.
\begin{center}
$\mathbf{E}_k^R=\mathbf{USV}^T$
\end{center}
After this, we could take $\mathbf{d}_k^\prime$ to be the first column of $\mathbf{U}$ and $\mathbf{x}_R^k$ as $\mathbf{S}(1,1)*\mathbf{V}(:,1)^T$, ie the first column of $\mathbf{V}$ multiplied by the top left element of $\mathbf{S}$, written as a row.
\subsection{The summary}
The dictionary may be initialized with the examples. The dictionary update step goes over the columns, updating them one by one and always using the latest values. This accelerates convergence. If old values are used (for example, if we want to run this as a parallel algorithm) then the convergence is around 4 times slower. After one run through the columns of the dictionary, the coefficients are adjusted by a pursuit algorithm, and this repeats over and over, till some convergence criteria.
\section{Pursuit algorithms to get sparse coefficients}
There exist many algorithms, like MP, OMP, FOCUSS, etc. Here is the MP algorithm.
\begin{verbatim}
set R as y; // The signal to be represented as a sparse combination
for i = 1:k // k is the number of non-zero coefficients allowed
    get the column d from dictionary having maximum |<d, R>|;
    set a(i) as <d, R>;
    set b(i) as index of d in the dictionary;
    set R as R - a(i)*d;
end for
\end{verbatim} 
\begin{texttt}a(i)\end{texttt} contains the coefficients and \begin{texttt}b(i)\end{texttt} contains the indices of the corresponding atom in the dictionary. 
\section{Switching to the kernel}
The above can be extended to kernel based dictionaries. Lets assume that we have a kernel mapping $\Phi$, which maps the given data to a different space (possibly having infinite dimensions). Now, we try to get a sparse representation using a dictionary in this space. Hence, the problem can now be formulated as 
\begin{center}
$\genfrac{}{}{0pt}{}{argmin}{\mathbf{D},\mathbf{X}}\{\|\Phi\left(\mathbf{Y}\right)-\mathbf{DX}\|_F^2\}$ subject to $\forall i, \|\mathbf{x}_i\|_0 \leq T_0$
\end{center}
Since the dictionary atoms would lie in the space of $\Phi\left(\mathbf{Y}\right)$, we can replace $\mathbf{D}$ by $\Phi\left(\mathbf{Y}\right)\mathbf{A}$, and we modify $\mathbf{A}$ to adapt the dictionary. Note that A has dimension $n\times n$, where $n$ is the number of examples. Hence the problem can be now stated as 
\begin{center}
$\genfrac{}{}{0pt}{}{argmin}{\mathbf{A},\mathbf{X}}\{\|\Phi\left(\mathbf{Y}\right)-\Phi\left(\mathbf{Y}\right)\mathbf{AX}\|_F^2\}$ subject to $\forall i, \|\mathbf{x}_i\|_0 \leq T_0$
\end{center}
\subsection{Updating X}
The above mentioned MP algorithm can be easily adapted to kernels. When we use kernels, instead of an explicit mapping $\Phi$, we replace the inner product in the input space with a Mercer kernel. It can be shown that this corresponds to mapping the data with some $\Phi$ into some feature space. Here is the modified algorithm
\begin{verbatim}
for i = 1:k // k is the number of non-zero coefficients allowed
    d0 = column from dictionary having maximum |K(d, y)-sum{j<i}(a(j)*K(d, D(b(j)))|;
    set a(i) as K(d0, R);
    set b(i) as index of d0 in the dictionary;
end for
\end{verbatim}
\subsection{Updating D}
The error can be re-written as 
\begin{center}
$\|\Phi\left(\mathbf{Y}\right)\mathbf{E}_k-\Phi\left(\mathbf{Y}\right)\mathbf{a}_k\mathbf{x}_T^k\|_F^2$
\end{center}
where $\mathbf{E}_k$ is $\left(\mathbf{I}-\sum_{j\neq k}\mathbf{a}_j\mathbf{x}_T^j\right)$.
This is similar to the earlier case. We again consider only the examples that use the $k^{th}$ atom in the current representation. Define $\mathbf{E}_k^R$ and $\mathbf{x}_R^k$ as earlier.  Consider
\begin{center}
$\Phi\left(\mathbf{Y}\right)\mathbf{E}_k^R = \mathbf{USV}^T$ and $\Phi\left(\mathbf{Y}\right)\mathbf{a}_k\mathbf{x}_R^k=\sigma_1\mathbf{u}_1\mathbf{v}_1^T$
\end{center}
As earlier, consider the following assignment for $\mathbf{x}_R^k$ and $\mathbf{a}_k$
\begin{center}
$\mathbf{x}_R^k = \sigma_1\mathbf{v}_1^T$
$\mathbf{a}_k = \mathbf{u}_1$ 
\end{center}
This ensures that the dictionary atoms are normalized in the kernel space. 
One issue here is that we cant do SVD directly because $\mathbf{E}_k^R$ may have infinite row dimension. We use the fact that SVD of a matrix $\mathbf{Q}$ is related to the eigen decomposition of $\mathbf{Q}^T\mathbf{Q}$. It is clear that
\begin{center}
$\left(\Phi\left(\mathbf{Y}\right)\mathbf{E}_k^R\right)^T\left(\Phi\left(\mathbf{Y}\right)\mathbf{E}_k^R\right)=\left(\mathbf{E}_k^R\right)^T\mathbb{K}\left(\mathbf{Y},\mathbf{Y}\right)\left(\mathbf{E}_k^R\right)=\mathbf{V}\Delta\mathbf{V}^T$
\end{center}
This gives us $\sigma_1=\sqrt{\Delta\left(1,1\right)}$ and $\mathbf{v}_1$ as the first column of $\mathbf{V}$. Thus, we have $\mathbf{x}_R^k$. After some algebraic manipulations, it can be seen that $\mathbf{a}_k=\sigma_1^{-1}\mathbf{E}_k^R\mathbf{v}_1$. This completes the description of the two phase kernel-SVD algorithm.
\section{Noise removal using K-SVD}
Noise removal is a potential application of K-SVD. We have a collection of noise-free images which serve as our model. We break the image into small patches, say 8x8, and build a dictionary of these 8x8 patches. The number of dictionary atoms is a free parameter that needs to be chosen. Very large numbers add up to the computational complexity. After the dictionary is trained using these model images, we try to get a sparse representation of image patches using this dictionary, in the hope that this would result in a noise-free image. 
\end{document}